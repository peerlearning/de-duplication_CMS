---
title: "De-duplicating CMS repositry"
author: "Pritesh Shrivastava"
date: "January 22, 2018"
output:
  pdf_document: default
  html_document: default
---

## Reading data from CMS

```{r echo = FALSE}
library(tidyverse)

Qs <- read_tsv("qs_topicwise_dump.tsv")

head(Qs)
```

### Cleaning and adding Grade, Subject, Curriculum and Chapter No 

```{r echo = FALSE}
# Need to clean the overflow of text
Qs_clean <- Qs  %>%
  dplyr::select(1:9) %>%                    # Keeping only the first 9 columns
  filter(!is.na(Difficulty))  %>%           # Cleaning overflow
  mutate(Grade = str_sub(Topic_Code, 5, 6), Subject = str_sub(Topic_Code, 1, 3), Curriculum = str_sub(Topic_Code, 8, 10), Ch_No = str_sub(Topic_Code, 12, 13))

head(Qs_clean)

```


## Creating vocabulary by tokenizing text

```{r echo = FALSE}
library(text2vec)

test <- Qs_clean %>% filter(Grade == 11, Subject == "BIO", Curriculum == "CBS", Ch_No =="05")
  
clean <- test$Text %>%
    str_to_lower
    # gsub is reducing the no of terms, need to find another way to remove unnecessary characters
    #{gsub("<.*?>", "", .)} %>%
    #{gsub("[^a-zA-Z\\s]", "", .)} %>% 
    #{gsub("//d", "",.)} %>%
    #{gsub("[:punct:]","", .)}
    
it = itoken(clean, progressbar = FALSE)

v = create_vocabulary(it) %>% prune_vocabulary(doc_proportion_max = 0.1, term_count_min = 3)

head(v)
```

## Creating Document Term Matices

```{r echo = FALSE}
vectorizer = vocab_vectorizer(v)

dtm = create_dtm(it, vectorizer)
dim(dtm)

```

### Cosine similarity with tf-idf

```{r echo = FALSE}
tfidf = TfIdf$new()
dtm_tfidf = fit_transform(dtm, tfidf)
dtm_tfidf[1:5, 1:5]
```

Calculate similarities between all rows of dtm_tfidf matrix

```{r echo = FALSE}
tfidf_cos_sim = sim2(x = dtm_tfidf, method = "cosine", norm = "l2")
tfidf_cos_sim[1:6, 1:6]

```

## Cosine similarity with Latent Semantic Analysis

Usually tf-idf/bag-of-words matrices contain a lot of noise. Applying LSA model can help with this problem, so you can achieve better quality similarities

```{r echo = FALSE}
lsa = LSA$new(n_topics = 10)
dtm_tfidf_lsa = fit_transform(dtm_tfidf, lsa)
```

Calculate similarities between all rows of dtm_tfidf_lsa matrix

```{r echo = FALSE}
tfidf_lsa_cos_sim = sim2(x = dtm_tfidf_lsa, method = "cosine", norm = "l2")
tfidf_lsa_cos_sim[1:6, 1:6]

```

### Tidying similarity matrix

```{r echo=FALSE}
similarity <- tfidf_lsa_cos_sim %>% as.data.frame.table()
head(similarity)

```

### Filtering near duplicates

```{r echo=FALSE}
# Figure out the optimum similarity threshold
dupl_list <- filter(similarity, Freq > .95) %>%
  rename( row = Var1, column = Var2, sim = Freq) %>% 
  filter(as.numeric(row) > as.numeric(column))  %>%   # Remove double entries
  filter(row != column)       # Remove diagnol entries
  
head(dupl_list)
```

## Final list of Duplicate Qs

```{r echo=FALSE}
duplicates <- dupl_list %>% mutate(row_id = test$Code[row],row_q = test$Text[row], row_status = test$Status[row],col_id = test$Code[column],  col_q = test$Text[column], col_status = test$Status[column])

duplicates[c(1:5),c(4,5,7,8)]

```

